{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from old.fastai.conv_learner import *\n",
    "from old.fastai.dataset import *\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir('C:/Users/User/Desktop/Avshalom&Naama/jupyter_files/fastai-master/')\n",
    "#os.listdir('C:/Users/User/Desktop/Avshalom&Naama/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#paths\n",
    "PATH = 'C:/Users/User/Desktop/Avshalom&Naama/jupyter_files/fastai-master/'\n",
    "TRAIN = '../../data/train_v2/'\n",
    "TEST = '../../data/test_v2/'\n",
    "SEGMENTATION = '../../data/train_ship_segmentations_v2.csv'\n",
    "PRETRAINED = '../../models/ResNet34_384/Resnet34_lable_384_1.h5'\n",
    "\n",
    "corrupted_image_train = '6384c3e78.jpg' #a corrupted image in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "nw = 2   #number of workers for data loader -  the number of CPUs to use, maybe in windows shuld be = 0 - \n",
    "# check in https://forums.fast.ai/t/difference-between-setting-num-workers-0-in-fastai-pytorch/40040\n",
    "arch = resnet34 #specify target architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names = [f for f in os.listdir(TEST)]\n",
    "#train_names.remove(corrupted_image_train) #remove corrupted image from train\n",
    "#5% of data in the validation set is sufficient for model evaluation (checked)\n",
    "tr_n, val_n = train_test_split(train_names, test_size=0.05, random_state=42)\n",
    "#get segmentation of ground truth\n",
    "segmentation_df = pd.read_csv(os.path.join(PATH, SEGMENTATION)).set_index('ImageId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is unbalanced - there are more images with no ships and if there are ships, the masked ships are a small precentege of pixels in the image. Therefore, we dropped all images with no ships to balance the training set. It also reduces the time per each epoch.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# if in ground truth there is no segment (no ship), remove from train set and validation set - str represents ship\n",
    "def cut_empty(names):\n",
    "    return [name for name in names \n",
    "            if(type(segmentation_df.loc[name]['EncodedPixels']) != float)]\n",
    "\n",
    "#removing no ship\n",
    "tr_n = cut_empty(tr_n)\n",
    "val_n = cut_empty(val_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# get mask of image from the data file\n",
    "def get_mask(img_id, df):\n",
    "    shape = (768,768)\n",
    "    #make array of zeroes for mask\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    #read masks of image from data file\n",
    "    masks = df.loc[img_id]['EncodedPixels']\n",
    "    #for image with no ship mask (float) return zeros image in the original image shape\n",
    "    if(type(masks) == float): return img.reshape(shape)\n",
    "    #for image with ships masks (str) make new array of masks (diffrent ships) \n",
    "    if(type(masks) == str): masks = [masks]\n",
    "    #for every mask of ship in image mark the mask as 1\n",
    "    for mask in masks:\n",
    "        s = mask.split()\n",
    "        for i in range(len(s)//2):\n",
    "            start = int(s[2*i]) - 1\n",
    "            length = int(s[2*i+1])\n",
    "            img[start:start+length] = 1\n",
    "    # return reshaped mask\n",
    "    return img.reshape(shape).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracts images and mask in shape 768X768 from file. inherits from FilesDataset class in fastsai - used by data loader\n",
    "class pdFilesDataset(FilesDataset):\n",
    "    def __init__(self, fnames, path, transform):\n",
    "        self.segmentation_df = pd.read_csv(SEGMENTATION).set_index('ImageId')\n",
    "        super().__init__(fnames, transform, path)\n",
    "    \n",
    "    #get image\n",
    "    def get_x(self, i):\n",
    "        img = open_image(os.path.join(self.path, self.fnames[i]))\n",
    "        # if maximum size of image in data set is 768X768 - resize the image to maximum shape\n",
    "        if self.sz == 768: return img \n",
    "        else: return cv2.resize(img, (self.sz, self.sz))\n",
    "    \n",
    "    #get mask\n",
    "    def get_y(self, i):\n",
    "        #if test set - get empty mask, else ground truth\n",
    "        mask = np.zeros((768,768), dtype=np.uint8) if (self.path == TEST)             else get_mask(self.fnames[i], self.segmentation_df)\n",
    "        img = Image.fromarray(mask).resize((self.sz, self.sz)).convert('RGB')\n",
    "        return np.array(img).astype(np.float32)\n",
    "    \n",
    "    def get_c(self): return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation - prevents overfitting. Transforms = rotating, changing lighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#augmentation - lighting (because kaggle dont have the new version of fastai we added this implementation)\n",
    "class RandomLighting(Transform):\n",
    "    def __init__(self, b, c, tfm_y=TfmType.NO):\n",
    "        super().__init__(tfm_y)\n",
    "        self.b,self.c = b,c # b for balance, c for contrast  to adjust random picture lighting\n",
    "\n",
    "    def set_state(self):\n",
    "        self.store.b_rand = rand0(self.b)\n",
    "        self.store.c_rand = rand0(self.c)\n",
    "\n",
    "    def do_transform(self, x, is_y):\n",
    "        if is_y and self.tfm_y != TfmType.PIXEL: return x  #add this line to fix the bug\n",
    "        b = self.store.b_rand\n",
    "        c = self.store.c_rand\n",
    "        c = -1/(c-1) if c<0 else c+1\n",
    "        x = lighting(x, b, c)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader - creats data set and data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sz,bs):\n",
    "    #data augmentation \n",
    "    aug_tfms = [RandomRotate(20, tfm_y=TfmType.CLASS), # x degree random rotation\n",
    "                RandomDihedral(tfm_y=TfmType.CLASS), # Rotates images by random multiples of 90 deg and/or reflection (flips)\n",
    "                RandomLighting(0.05, 0.05, tfm_y=TfmType.CLASS)] # random picture lighting\n",
    "    #resizing, image cropping, initial normalization, needs the pretraind arch to know normalization way\n",
    "    tfms = tfms_from_model(arch, sz, crop_type=CropType.NO, tfm_y=TfmType.CLASS, \n",
    "                aug_tfms=aug_tfms)\n",
    "    tr_names = tr_n if (len(tr_n)%bs == 0) else tr_n[:-(len(tr_n)%bs)] #cut incomplete batch\n",
    "    #import dataset (train,test,validation) from data file with transforms (augmentations) \n",
    "    ds = ImageData.get_ds(pdFilesDataset, (tr_names,TRAIN), \n",
    "                (val_n,TRAIN), tfms, test=(test_names,TEST))\n",
    "    # import model data\n",
    "    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n",
    "    #md.is_multi = False\n",
    "    return md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "The U-net model is composed of a ResNet34 based encoder and a simple upsampling decoder. Skip connections are added between encoder and decoder to facilitate the information flow at different detalization levels. Meanwhile, using a pretrained ResNet34 model allows us to have a powerful encoder capable of handling elaborated feature, in comparison with the original U-net, without a risk of overfitting and necessity of training a big model from scratch. Before using, the original ResNet34 model was further fine-tuned on ship/no-ship classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives the meta data of the architecture - which layers to cut from resnet to do transfer learning\n",
    "cut,lr_cut = model_meta[arch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ResNet34 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#get the relevent layers from the pretrained model\n",
    "def get_base():                  \n",
    "    layers = cut_model(arch(True), cut)\n",
    "    #nn.Sequential builds a neural net by specifying sequentially the building blocks of the net\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "#load a model pretrained on ship/no-ship classification\n",
    "def load_pretrained(model, path): \n",
    "    weights = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "    model.load_state_dict(weights, strict=False)     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of unet blocks\n",
    "class UnetBlock(nn.Module):\n",
    "    #up_in - number of channels for upsampeling  \n",
    "    #x_in - number of channels of the activations features from an intermediate layer of the encoder (the connection)\n",
    "    #n_out - number of out channels\n",
    "    def __init__(self, up_in, x_in, n_out):\n",
    "        super().__init__()\n",
    "        up_out = x_out = n_out//2\n",
    "        # set 1X1 convolution func for reducing the number of channels  \n",
    "        self.x_conv  = nn.Conv2d(in_channels=x_in, out_channels=x_out, kernel_size=1)\n",
    "        #set upsampling func - Applies a 2x2 transposed convolution operator over an input image\n",
    "        self.tr_conv = nn.ConvTranspose2d(in_channels=up_in, out_channels=up_out, kernel_size=2, stride=2)\n",
    "        #set batch normalization func\n",
    "        self.bn = nn.BatchNorm2d(n_out)\n",
    "    #up_p - downsampled input\n",
    "    #x_p - the activation from the encoder\n",
    "    def forward(self, up_p, x_p):\n",
    "        #upsample\n",
    "        up_p = self.tr_conv(up_p)\n",
    "        #decrease channels from activation\n",
    "        x_p = self.x_conv(x_p)\n",
    "        #concat the upsampled with activation\n",
    "        cat_p = torch.cat([up_p,x_p], dim=1)\n",
    "        #return after relu\n",
    "        return self.bn(F.relu(cat_p))\n",
    "\n",
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = output\n",
    "    def remove(self): self.hook.remove()\n",
    "\n",
    "#the net   \n",
    "class Unet34(nn.Module):\n",
    "    def __init__(self, rn):\n",
    "        super().__init__()\n",
    "        #rn - the ResNet\n",
    "        self.rn = rn\n",
    "        self.sfs = [SaveFeatures(rn[i]) for i in [2,4,5,6]]\n",
    "        self.up1 = UnetBlock(512,256,256)\n",
    "        self.up2 = UnetBlock(256,128,256)\n",
    "        self.up3 = UnetBlock(256,64,256)\n",
    "        self.up4 = UnetBlock(256,64,256)\n",
    "        self.up5 = nn.ConvTranspose2d(256, 1, 2, stride=2)\n",
    "       \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.rn(x))\n",
    "        x = self.up1(x, self.sfs[3].features)\n",
    "        x = self.up2(x, self.sfs[2].features)\n",
    "        x = self.up3(x, self.sfs[1].features)\n",
    "        x = self.up4(x, self.sfs[0].features)\n",
    "        x = self.up5(x)\n",
    "        return x[:,0]\n",
    "    \n",
    "    def close(self):\n",
    "        for sf in self.sfs: sf.remove()\n",
    "            \n",
    "class UnetModel():\n",
    "    def __init__(self,model,name='Unet'):\n",
    "        self.model,self.name = model,name\n",
    "\n",
    "    def get_layer_groups(self, precompute):\n",
    "        lgs = list(split_by_idxs(children(self.model.rn), [lr_cut]))\n",
    "        return lgs + [children(self.model)[1:]]\n",
    "\n",
    "\n",
    "# <h1><center>Loss function</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def dice_loss(input, target):\n",
    "    input = torch.sigmoid(input)\n",
    "    smooth = 1.0\n",
    "\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    \n",
    "    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val +             ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class MixedLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.focal = FocalLoss(gamma)\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(pred, targs):\n",
    "    pred = (pred>0).float()\n",
    "    return 2.0 * (pred*targs).sum() / ((pred+targs).sum() + 1.0)\n",
    "\n",
    "def IoU(pred, targs):\n",
    "    pred = (pred>0).float()\n",
    "    intersection = (pred*targs).sum()\n",
    "    return intersection / ((pred+targs).sum() - intersection + 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "m_base = load_pretrained(get_base(),PRETRAINED)\n",
    "m = to_gpu(Unet34(m_base))\n",
    "models = UnetModel(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (256x256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "sz = 256 #image size\n",
    "bs = 64 #batch size\n",
    "\n",
    "md = get_data(sz,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "learn = ConvLearner(md, models)\n",
    "learn.opt_fn=optim.Adam\n",
    "learn.crit = MixedLoss(10.0, 2.0)\n",
    "learn.metrics=[accuracy_thresh(0.5),dice,IoU]\n",
    "wd=1e-7\n",
    "lr = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "learn.freeze_to(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#learn.fit(lr,1,wds=wd,cycle_len=1,use_clr=(5,8))\n",
    "learn.fit(lr,2,wds=wd,cycle_len=1,use_clr=(5,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "learn.save('Unet34_256_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "lrs = np.array([lr/100,lr/10,lr])\n",
    "learn.unfreeze() #unfreeze the encoder\n",
    "learn.bn_freeze(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#learn.fit(lrs,2,wds=wd,cycle_len=1,use_clr=(20,8))\n",
    "learn.fit(lrs,6,wds=wd,cycle_len=1,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#learn.fit(lrs/3,2,wds=wd,cycle_len=2,use_clr=(20,8))\n",
    "learn.fit(lrs/3,3,wds=wd,cycle_len=2,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "learn.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('Unet34_256_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization (256x256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def Show_images100(x,yp,yt):\n",
    "    columns = 3\n",
    "    rows = min(bs,8)\n",
    "    fig=plt.figure(figsize=(columns*4, rows*4))\n",
    "    for i in range(rows):\n",
    "        fig.add_subplot(rows, columns, 3*i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(x[i])\n",
    "        fig.add_subplot(rows, columns, 3*i+2)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(yp[i])\n",
    "        fig.add_subplot(rows, columns, 3*i+3)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(yt[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "learn.model.eval();\n",
    "x,y = next(iter(md.val_dl))\n",
    "yp = to_np(F.sigmoid(learn.model(V(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Show_images(np.asarray(md.val_ds.denorm(x)), yp, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (384x384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "sz = 384 #image size\n",
    "bs = 16 #original 32  #batch size\n",
    "\n",
    "md = get_data(sz,bs)\n",
    "learn.set_data(md)\n",
    "learn.unfreeze()\n",
    "learn.bn_freeze(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#learn.fit(lrs/5,1,wds=wd,cycle_len=2,use_clr=(10,8))\n",
    "learn.fit(lrs/5,4,wds=wd,cycle_len=2,use_clr=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('Unet34_384_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization (384x384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "learn.model.eval();\n",
    "x,y = next(iter(md.val_dl))\n",
    "yp = to_np(F.sigmoid(learn.model(V(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Show_images(np.asarray(md.val_ds.denorm(x)), yp, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (768x768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "sz = 768 #image size\n",
    "bs = 6  #batch size\n",
    "\n",
    "md = get_data(sz,bs)\n",
    "learn.set_data(md)\n",
    "learn.unfreeze()\n",
    "learn.bn_freeze(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#learn.fit(lrs/10,1,wds=wd,cycle_len=1,use_clr=(10,8))\n",
    "learn.fit(lrs/10,5,wds=wd,cycle_len=1,use_clr=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('Unet34_768_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization (768x768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "learn.model.eval();\n",
    "x,y = next(iter(md.val_dl))\n",
    "yp = to_np(F.sigmoid(learn.model(V(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "Show_images(np.asarray(md.val_ds.denorm(x)), yp, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
